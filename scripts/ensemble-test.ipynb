{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ca12xrlwmkQ4"
   },
   "source": [
    "# **Navigation/Mount/Env**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19770,
     "status": "ok",
     "timestamp": 1578674539196,
     "user": {
      "displayName": "Anindya SHAHA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAZ1FVPZRjiasffSqabig5t8Fu4FpcUYUzJfmsf=s64",
      "userId": "14305286849493684106"
     },
     "user_tz": -60
    },
    "id": "Hlx6e3puMYAH",
    "outputId": "1ad92857-0901-49dd-e5cf-df27fb9a14fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n",
      "Collecting efficientnet\n",
      "  Downloading https://files.pythonhosted.org/packages/97/82/f3ae07316f0461417dc54affab6e86ab188a5a22f33176d35271628b96e0/efficientnet-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: keras-applications<=1.0.8,>=1.0.7 in /tensorflow-2.1.0/python3.6 (from efficientnet) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.6/dist-packages (from efficientnet) (0.16.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /tensorflow-2.1.0/python3.6 (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /tensorflow-2.1.0/python3.6 (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (6.2.2)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (2.4)\n",
      "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (2.4.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (3.1.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /tensorflow-2.1.0/python3.6 (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->efficientnet) (4.4.1)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.4.6)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.6.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /tensorflow-2.1.0/python3.6 (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (44.0.0)\n",
      "Installing collected packages: efficientnet\n",
      "Successfully installed efficientnet-1.0.0\n",
      "Collecting image-classifiers==0.2.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/7f/31234ee4bc8243f9e8b59b827e8a61436d7269cf75936b0aecc48a08f06c/image_classifiers-0.2.2-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 2.7MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: keras>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from image-classifiers==0.2.2) (2.2.5)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /tensorflow-2.1.0/python3.6 (from keras>=2.1.0->image-classifiers==0.2.2) (1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.0->image-classifiers==0.2.2) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /tensorflow-2.1.0/python3.6 (from keras>=2.1.0->image-classifiers==0.2.2) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /tensorflow-2.1.0/python3.6 (from keras>=2.1.0->image-classifiers==0.2.2) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /tensorflow-2.1.0/python3.6 (from keras>=2.1.0->image-classifiers==0.2.2) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /tensorflow-2.1.0/python3.6 (from keras>=2.1.0->image-classifiers==0.2.2) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.0->image-classifiers==0.2.2) (1.4.1)\n",
      "Installing collected packages: image-classifiers\n",
      "Successfully installed image-classifiers-0.2.2\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU Integration\n",
    "%tensorflow_version 2.x \n",
    "%pip install -U --pre efficientnet\n",
    "%pip install -U image-classifiers==0.2.2\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('Alert: GPU Device Not Found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39167,
     "status": "ok",
     "timestamp": 1578674558601,
     "user": {
      "displayName": "Anindya SHAHA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAZ1FVPZRjiasffSqabig5t8Fu4FpcUYUzJfmsf=s64",
      "userId": "14305286849493684106"
     },
     "user_tz": -60
    },
    "id": "RgRSx8HclBQV",
    "outputId": "5fc38197-c894-42ee-921a-a5bb34134c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39542,
     "status": "ok",
     "timestamp": 1578674558984,
     "user": {
      "displayName": "Anindya SHAHA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAZ1FVPZRjiasffSqabig5t8Fu4FpcUYUzJfmsf=s64",
      "userId": "14305286849493684106"
     },
     "user_tz": -60
    },
    "id": "lkjlLZPMl2c1",
    "outputId": "31e0c798-2022-4cec-89d6-a0aa0138fd81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/projects/dermo/scripts\n"
     ]
    }
   ],
   "source": [
    "# Project Folder\n",
    "%cd \"gdrive/My Drive/projects/dermo/scripts/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44552,
     "status": "ok",
     "timestamp": 1578674564001,
     "user": {
      "displayName": "Anindya SHAHA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAZ1FVPZRjiasffSqabig5t8Fu4FpcUYUzJfmsf=s64",
      "userId": "14305286849493684106"
     },
     "user_tz": -60
    },
    "id": "a4JYfooFmGb1",
    "outputId": "dd781393-0cc8-46b8-c1e8-7585ee0f9079"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color-io.ipynb       ensemble-val.ipynb  train-val.ipynb\n",
      "ensemble-test.ipynb  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "# Current Directory\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45938,
     "status": "error",
     "timestamp": 1578674565390,
     "user": {
      "displayName": "Anindya SHAHA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAZ1FVPZRjiasffSqabig5t8Fu4FpcUYUzJfmsf=s64",
      "userId": "14305286849493684106"
     },
     "user_tz": -60
    },
    "id": "MpqPh3fCd-h8",
    "outputId": "42aff20d-9389-4692-8a16-969258537b9a"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import filters as skifilters\n",
    "from scipy import ndimage\n",
    "from skimage import filters\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import dill\n",
    "from sklearn.metrics import (roc_curve, auc, accuracy_score, f1_score, precision_score, \n",
    "                             recall_score, classification_report, confusion_matrix)\n",
    "from scipy import interp\n",
    "import efficientnet.tfkeras as efn\n",
    "from classification_models.tfkeras import Classifiers as aux_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LY5UgBbHGQ0C"
   },
   "source": [
    "# **I/O**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rJEphnawFqe"
   },
   "source": [
    "## **● Load NPY Test Dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 640,
     "status": "ok",
     "timestamp": 1578419545759,
     "user": {
      "displayName": "Anindya SHAHA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAZ1FVPZRjiasffSqabig5t8Fu4FpcUYUzJfmsf=s64",
      "userId": "14305286849493684106"
     },
     "user_tz": -60
    },
    "id": "qIvWItYKvl7Y",
    "outputId": "7fec04e8-ee97-406a-c720-b8409bf9ca35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Test Images: 1015\n"
     ]
    }
   ],
   "source": [
    "# Testing Dataset Parameters\n",
    "TEST_DIR    = '../data/grayworld/test/'    \n",
    "TEST_MODE   = 'val'                      # 'val'/'test'\n",
    "\n",
    "def findScan(data, name, key):\n",
    "  for i, dic in data.items():\n",
    "    if dic[key] == name:        \n",
    "      return i\n",
    "    return -1\n",
    "\n",
    "def test_io(path,mode):\n",
    "  \"\"\"\"\n",
    "  Input:  path\n",
    "  Output: data[p] = {\n",
    "      'id':\n",
    "      'image':\n",
    "      'label': }\n",
    "  \"\"\"\n",
    "  if (mode=='test'):\n",
    "    # Importing Images\n",
    "    target_dir  = glob.glob(path+\"/*.png\")\n",
    "    print(\"Number of Test Images:\", len(target_dir))\n",
    "    \n",
    "    # Creating Dictionary\n",
    "    data = {}\n",
    "    for p in range(len(target_dir)):\n",
    "        scan_id  =  target_dir[p].replace(\".png\", \"\")\n",
    "        scan_id  =  scan_id.replace(path, \"\")\n",
    "        # Creating List of Dictionary                    \n",
    "        data[p] = {'id'    : scan_id,\n",
    "                   'image' : target_dir[p] }\n",
    "  \n",
    "  elif (mode=='val'):\n",
    "    # Importing Images\n",
    "    les_dir = glob.glob(path+\"/les/*.png\")\n",
    "    nv_dir  = glob.glob(path+\"/nv/*.png\")\n",
    "    print(\"Number of LES Images:\", len(les_dir))\n",
    "    print(\"Number of NV Images:\",  len(nv_dir))\n",
    "    \n",
    "    # Creating Dictionary (LES Scans)\n",
    "    data = {}\n",
    "    for p in range(len(les_dir)):\n",
    "      scan_id  =  les_dir[p].replace(\".png\", \"\")\n",
    "      scan_id  =  scan_id.replace(path+\"/les\\\\\", \"\")\n",
    "      # Creating List of Dictionary                    \n",
    "      data[p] = { 'id'    : scan_id,\n",
    "                  'image' : les_dir[p],\n",
    "                  'label' : 1 }                         # Label of LES = 1\n",
    "\n",
    "    # Creating Dictionary (NV Scans)\n",
    "    for p in range(len(nv_dir)):\n",
    "      scan_id  =  nv_dir[p].replace(\".png\", \"\")\n",
    "      scan_id  =  scan_id.replace(path+\"/nv\\\\\", \"\")\n",
    "      # Creating List of Dictionary                    \n",
    "      data[p+len(les_dir)] = { 'id'    : scan_id,\n",
    "                               'image' : nv_dir[p],\n",
    "                               'label' : 0 }            # Label of NV = 0\n",
    "  return data\n",
    "\n",
    "# Load Testing Dataset for Full Inference\n",
    "test_dataset   = test_io(path=TEST_DIR, mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qoSV85H3vaq"
   },
   "source": [
    "# **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MwiDvhWN1Cc"
   },
   "outputs": [],
   "source": [
    "# Load Pre-Trained Models\n",
    "E01          = tf.keras.models.load_model('../fin/E01.h5')         # EfficientNet-B6\n",
    "# E01          = tf.keras.models.load_model('../fin/E02.h5')         # Inception-V3\n",
    "# E01          = tf.keras.models.load_model('../fin/E03.h5')         # SENet-154\n",
    "# E01          = tf.keras.models.load_model('../fin/E04.h5')         # SEResNeXt-101\n",
    "# E01          = tf.keras.models.load_model('../fin/E05.h5')         # DenseNet-169\n",
    "\n",
    "# Inference Parameters\n",
    "TTA          = True                                                # Test-Time Augmentation\n",
    "TTA_MODE     ='mean'                                               # 'mean'/'maxconf'\n",
    "IO_X         = 450                                                 # Original Dimensions\n",
    "IO_Y         = 600                                                 # Original Dimensions\n",
    "DIM_X        = 224                                                 # Cropped Training Dimensions\n",
    "DIM_Y        = 224                                                 # Cropped Training Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 669350,
     "status": "ok",
     "timestamp": 1578424708370,
     "user": {
      "displayName": "Anindya SHAHA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAZ1FVPZRjiasffSqabig5t8Fu4FpcUYUzJfmsf=s64",
      "userId": "14305286849493684106"
     },
     "user_tz": -60
    },
    "id": "HwOJCcxePHCw",
    "outputId": "5134337e-3232-41e7-ac94-31825a3b9918"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1015/1015 [11:07<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Populate Member Predictions on Testing Dataset\n",
    "E01_predictions   = np.zeros((1,1))\n",
    "all_scan_ids      = np.zeros((1,1))\n",
    "\n",
    "for index in tqdm.tqdm(range(0,len(test_dataset))):\n",
    "  # Load Data\n",
    "  image           = plt.imread(test_dataset[index]['image'])\n",
    "  scan_id         = test_dataset[index]['id']\n",
    "\n",
    "  # Crop/Resize to Training Dimensions as Tensors\n",
    "  image_tensor    = tf.constant(image, tf.float32)\n",
    "  image_tensor    = tf.image.crop_to_bounding_box(image_tensor,(IO_X-DIM_X)//2,                        \n",
    "                                                               (IO_Y-DIM_Y)//2,\n",
    "                                                                DIM_X,DIM_Y) \n",
    "  if (TTA==True):\n",
    "    # Test-Time Augmentation\n",
    "    image01_tensor  = tf.clip_by_value(tf.image.flip_left_right(image_tensor),0.0,1.0)                            # TTA 01: Horizontal Flip\n",
    "    image02_tensor  = tf.clip_by_value(tf.image.flip_up_down(image_tensor),0.0,1.0)                               # TTA 02: Vertical Flip\n",
    "    image03_tensor  = tf.clip_by_value(tf.image.adjust_brightness(image_tensor,delta=16.0/255.0),0.0,1.0)         # TTA 03: Brightness Shift\n",
    "    image04_tensor  = tf.clip_by_value(tf.image.adjust_saturation(image_tensor,saturation_factor=1.5),0.0,1.0)    # TTA 04: Saturation Shift\n",
    "    image05_tensor  = tf.clip_by_value(tf.image.adjust_contrast(image_tensor,contrast_factor=1.5),0.0,1.0)        # TTA 05: Contrast Boost\n",
    "    # All Predictions\n",
    "    all_predictions = np.array((E01.predict(np.expand_dims(image_tensor.numpy(),axis=0)),\n",
    "                                E01.predict(np.expand_dims(image01_tensor.numpy(),axis=0)),\n",
    "                                E01.predict(np.expand_dims(image02_tensor.numpy(),axis=0)),\n",
    "                                E01.predict(np.expand_dims(image03_tensor.numpy(),axis=0)),\n",
    "                                E01.predict(np.expand_dims(image04_tensor.numpy(),axis=0)),\n",
    "                                E01.predict(np.expand_dims(image05_tensor.numpy(),axis=0))))\n",
    "    if (TTA_MODE=='mean'):\n",
    "      # Average Predictions\n",
    "      TTA_prediction  = np.average(all_predictions)\n",
    "    elif (TTA_MODE=='maxconf'):\n",
    "      # Max Confidence Prediction\n",
    "      maxconf_arg     = np.argmax((abs(0.5-all_predictions)))\n",
    "      TTA_prediction  = (all_predictions[maxconf_arg]).mean()\n",
    "    \n",
    "  elif (TTA==False):\n",
    "    TTA_prediction  = model.predict(np.expand_dims(image_tensor.numpy(),axis=0)) \n",
    "  \n",
    "  # Aggregate Predictions\n",
    "  E01_predictions   = np.concatenate((E01_predictions,TTA_prediction.reshape(-1,1)), axis=0)\n",
    "  all_scan_ids      = np.concatenate((all_scan_ids,np.array(scan_id).reshape(-1,1)), axis=0)\n",
    "\n",
    "E01_predictions = E01_predictions[1:]\n",
    "all_scan_ids    = all_scan_ids[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d8t0aYBaZMdU"
   },
   "outputs": [],
   "source": [
    "# Save Predictions\n",
    "predictions    =  pd.DataFrame(list(zip(all_scan_ids, E01_predictions)),\n",
    "columns        =  ['scan_id','prob'])\n",
    "predictions.to_csv('../test_pred/E01_testset.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ensemble-test",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
